{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "494e1e99-5353-4c25-b75a-873e8768cd93",
   "metadata": {},
   "source": [
    "# PROBLEM STATEMENT\n",
    "The primary objective of this work is to create a RAG pipeline over the `101 alpha formulaic book` to extract alpha number, expression and explanation. Utimately, this will serve as a starting point for creating an end-to-end alpha agent that will discover alpha signals over a knowledge base, create its python implementation, perform backtesting and trade live if expected performance pass a satisfactory threshold.\n",
    "\n",
    "Here we will be using GPT4o so as to better reduce hallucination and more control over the structure of the responses from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e39bde71-0e7f-4146-8d5b-9f24ddf1ba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers gpt4all sentence-transformers langchain langchain-core langchain-community langchain-openai faiss-cpu pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbde1227-84ae-49ee-a970-1ff23087bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Optional, add tracing in LangSmith\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Alpha agent project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21ab45f6-cf20-408e-a5a1-53827da1c6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Pass in LANGCHAIN_API_KEY ········\n",
      "Pass in OPENAI_API_KEY ········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "def _set_if_undefined(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass(f\"Pass in {var}\")\n",
    "\n",
    "_set_if_undefined(\"LANGCHAIN_API_KEY\")\n",
    "_set_if_undefined(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778c208e-ca1f-42c3-9686-cb8726f6c152",
   "metadata": {},
   "source": [
    "## Ground an alpha retriever to document provided\n",
    "\n",
    "The goal of this work is to generate alpha expression and explanation based on the document provided. If response not relevant to document, we return nothing.\n",
    "\n",
    "Based on the workflow above, the following steps are followed:\n",
    "1. Based on a given `query`, we generate relevant documents.\n",
    "2. We then use a `binary score` to check whether each retrieved document is relevant or not.\n",
    "3. __If document is relevant__, we generate an answer. otherwise, we `re-write` the `query`.\n",
    "4. For the answer generated in 3, we check if the answer is grounded to the document(to reduce hallucination).\n",
    "5. __If no hallucination__, we then check if `answer` is relevant to the question. otherwise `repeat step 3`.\n",
    "6. __If answer is relevant to question__, return answer. otherwise, `repeat 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeaf1a70-62e2-41c9-9505-d67fe39db2bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'model_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 23\u001b[0m\n\u001b[0;32m     11\u001b[0m doc_splits \u001b[38;5;241m=\u001b[39m splitter\u001b[38;5;241m.\u001b[39msplit_documents(documents)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# model_id = 'sentence-transformers/all-MiniLM-L6-v2'\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# model_kwargs = {'device': 'cpu'}\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# hf_embeddings = HuggingFaceEmbeddings(\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m \n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Add to vectorDB\u001b[39;00m\n\u001b[0;32m     21\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mfrom_documents(\n\u001b[0;32m     22\u001b[0m     documents\u001b[38;5;241m=\u001b[39mdoc_splits,\n\u001b[1;32m---> 23\u001b[0m     embedding\u001b[38;5;241m=\u001b[39m\u001b[43mGPT4AllEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     25\u001b[0m number_of_relevant_docs \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mceil(\u001b[38;5;241m0.8\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(doc_splits))\n\u001b[0;32m     26\u001b[0m retriever \u001b[38;5;241m=\u001b[39m vectorstore\u001b[38;5;241m.\u001b[39mas_retriever(search_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m: number_of_relevant_docs})\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\v1\\main.py:339\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mCreate a new model by parsing and validating input data from keyword arguments.\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03mRaises ValidationError if the input data cannot be parsed to form a valid model.\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;66;03m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[39;00m\n\u001b[1;32m--> 339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__pydantic_self__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\v1\\main.py:1100\u001b[0m, in \u001b[0;36mvalidate_model\u001b[1;34m(model, input_data, cls)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1100\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAssertionError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m   1102\u001b[0m     errors\u001b[38;5;241m.\u001b[39mappend(ErrorWrapper(exc, loc\u001b[38;5;241m=\u001b[39mROOT_KEY))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\embeddings\\gpt4all.py:39\u001b[0m, in \u001b[0;36mGPT4AllEmbeddings.validate_environment\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpt4all\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Embed4All\n\u001b[0;32m     38\u001b[0m     values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m Embed4All(\n\u001b[1;32m---> 39\u001b[0m         model_name\u001b[38;5;241m=\u001b[39m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[0;32m     40\u001b[0m         n_threads\u001b[38;5;241m=\u001b[39mvalues\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_threads\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     41\u001b[0m         device\u001b[38;5;241m=\u001b[39mvalues\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvalues\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt4all_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     43\u001b[0m     )\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import gpt4all library. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install the gpt4all library to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse this embedding model: pip install gpt4all\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     49\u001b[0m     )\n",
      "\u001b[1;31mKeyError\u001b[0m: 'model_name'"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import math\n",
    "\n",
    "loader = PyPDFLoader(\"../101-Alpha-Formula.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
    "doc_splits = splitter.split_documents(documents)\n",
    "\n",
    "# model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "# model_kwargs = {'device': 'cpu'}\n",
    "# hf_embeddings = HuggingFaceEmbeddings(\n",
    "#     model_name=model_id,\n",
    "#     model_kwargs=model_kwargs\n",
    "# )\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=GPT4AllEmbeddings(),\n",
    ")\n",
    "number_of_relevant_docs = math.ceil(0.8 * len(doc_splits))\n",
    "retriever = vectorstore.as_retriever(search_kwargs={'k': number_of_relevant_docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8050c8e8-16b7-4f92-a1c9-7971f40854d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing query to ensure no duplicate documents returned by retriever's get_relevant_documents method\n",
    "\n",
    "ALPHA_EXPRESSION_EXAMPLE = \"\"\"\n",
    "(rank((open - (sum(vwap, 10) / 10))) * (-1 * abs(rank((close - vwap))))) \\n\n",
    "((rank(Log(product(rank((rank(correlation(vwap, sum(adv10, 49.6054),\n",
    "8.47743))^4)), 14.9655))) < rank(correlation(rank(vwap), rank(volume), 5.07914))) * -1) \\n\n",
    "(max(rank(decay_linear(delta(((close * 0.369701) + (vwap * (1 - 0.369701))),\n",
    "1.91233), 2.65461)), Ts_Rank(decay_linear(abs(correlation(IndNeutralize(adv81,\n",
    "IndClass.industry), close, 13.4132)), 4.89768), 14.4535)) * -1) \\n\n",
    "\"\"\"\n",
    "\n",
    "STRATEGY = \"flow of funds strategy\"\n",
    "query = f\"\"\"Generate alpha information(s) based on a strategy \\n\n",
    "You are to respond with alpha information(alpha number, alpha expression and alpha explanation). \\n\n",
    "\n",
    "Here is the strategy: \\n\n",
    "{STRATEGY}\n",
    "\n",
    "Here is an example of alpha expression: \\n\n",
    "{ALPHA_EXPRESSION_EXAMPLE}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7c716d2e-86ad-4984-8acf-5c1bf1b83082",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b09bec96-d867-41be-9187-22bbb4863010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c89cd18b-38c4-4977-8646-f1a410baaaa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_duplicates(documents):\n",
    "    seen = set()\n",
    "    unique_docs = []\n",
    "    for doc in documents:\n",
    "        if doc.page_content not in seen:\n",
    "            unique_docs.append(doc)\n",
    "            seen.add(doc.page_content)\n",
    "    return unique_docs\n",
    "\n",
    "# Use the remove_duplicates function to remove duplicates from the retrieved documents\n",
    "unique_docs = remove_duplicates(docs)\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "06754590-7994-4696-b148-ce2ff56634fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='12 \\n Alpha#61:  (rank((vwap - ts_min(vwap, 16.1219))) < rank(corre lation(vwap, adv180, 17.9282))) \\nAlpha#62:  ((rank(correlation(vwap, sum(adv20, 22.4101), 9.91 009)) < rank(((rank(open) + \\nrank(open)) < (rank(((high + low) / 2)) + rank(high ))))) * -1) \\nAlpha#63:  ((rank(decay_linear(delta(IndNeutralize(close, Ind Class.industry), 2.25164), 8.22237)) \\n- rank(decay_linear(correlation(((vwap * 0.318108) + (open * (1 - 0.318108))), sum(adv180, \\n37.2467), 13.557), 12.2883))) * -1) \\nAlpha#64:  ((rank(correlation(sum(((open * 0.178404) + (low *  (1 - 0.178404))), 12.7054), \\nsum(adv120, 12.7054), 16.6208)) < rank(delta(((((hi gh + low) / 2) * 0.178404) + (vwap * (1 - \\n0.178404))), 3.69741))) * -1) \\nAlpha#65:  ((rank(correlation(((open * 0.00817205) + (vwap * (1 - 0.00817205))), sum(adv60, \\n8.6911), 6.40374)) < rank((open - ts_min(open, 13.6 35)))) * -1) \\nAlpha#66:  ((rank(decay_linear(delta(vwap, 3.51013), 7.23052) ) + Ts_Rank(decay_linear(((((low \\n* 0.96633) + (low * (1 - 0.96633))) - vwap) / (open  - ((high + low) / 2))), 11.4157), 6.72611)) * -1) \\nAlpha#67:  ((rank((high - ts_min(high, 2.14593)))^rank(correl ation(IndNeutralize(vwap, \\nIndClass.sector), IndNeutralize(adv20, IndClass.sub industry), 6.02936))) * -1) \\nAlpha#68:  ((Ts_Rank(correlation(rank(high), rank(adv15), 8.9 1644), 13.9333) < \\nrank(delta(((close * 0.518371) + (low * (1 - 0.5183 71))), 1.06157))) * -1) \\nAlpha#69:  ((rank(ts_max(delta(IndNeutralize(vwap, IndClass.i ndustry), 2.72412), \\n4.79344))^Ts_Rank(correlation(((close * 0.490655) +  (vwap * (1 - 0.490655))), adv20, 4.92416), \\n9.0615)) * -1) \\nAlpha#70:  ((rank(delta(vwap, 1.29456))^Ts_Rank(correlation(I ndNeutralize(close, \\nIndClass.industry), adv50, 17.8256), 17.9171)) * -1 ) \\nAlpha#71:  max(Ts_Rank(decay_linear(correlation(Ts_Rank(close , 3.43976), Ts_Rank(adv180, \\n12.0647), 18.0175), 4.20501), 15.6948), Ts_Rank(dec ay_linear((rank(((low + open) - (vwap + \\nvwap)))^2), 16.4662), 4.4388)) \\nAlpha#72:  (rank(decay_linear(correlation(((high + low) / 2),  adv40, 8.93345), 10.1519)) / \\nrank(decay_linear(correlation(Ts_Rank(vwap, 3.72469 ), Ts_Rank(volume, 18.5188), 6.86671), \\n2.95011))) \\nAlpha#73:  (max(rank(decay_linear(delta(vwap, 4.72775), 2.918 64)), \\nTs_Rank(decay_linear(((delta(((open * 0.147155) + ( low * (1 - 0.147155))), 2.03608) / ((open * \\n0.147155) + (low * (1 - 0.147155)))) * -1), 3.33829 ), 16.7411)) * -1)', metadata={'source': '101-Alpha-Formula.pdf', 'page': 11})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6340a220-13e0-4bda-8437-628dee6c3b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='14 \\n Alpha#87:  (max(rank(decay_linear(delta(((close * 0.369701) +  (vwap * (1 - 0.369701))), \\n1.91233), 2.65461)), Ts_Rank(decay_linear(abs(corre lation(IndNeutralize(adv81, \\nIndClass.industry), close, 13.4132)), 4.89768), 14. 4535)) * -1) \\nAlpha#88:  min(rank(decay_linear(((rank(open) + rank(low)) - (rank(high) + rank(close))), \\n8.06882)), Ts_Rank(decay_linear(correlation(Ts_Rank (close, 8.44728), Ts_Rank(adv60, \\n20.6966), 8.01266), 6.65053), 2.61957)) \\nAlpha#89:  (Ts_Rank(decay_linear(correlation(((low * 0.967285 ) + (low * (1 - 0.967285))), adv10, \\n6.94279), 5.51607), 3.79744) - Ts_Rank(decay_linear (delta(IndNeutralize(vwap, \\nIndClass.industry), 3.48158), 10.1466), 15.3012)) \\nAlpha#90:  ((rank((close - ts_max(close, 4.66719)))^Ts_Rank(c orrelation(IndNeutralize(adv40, \\nIndClass.subindustry), low, 5.38375), 3.21856)) * - 1) \\nAlpha#91:  ((Ts_Rank(decay_linear(decay_linear(correlation(In dNeutralize(close, \\nIndClass.industry), volume, 9.74928), 16.398), 3.83 219), 4.8667) - \\nrank(decay_linear(correlation(vwap, adv30, 4.01303) , 2.6809))) * -1) \\nAlpha#92:  min(Ts_Rank(decay_linear(((((high + low) / 2) + cl ose) < (low + open)), 14.7221), \\n18.8683), Ts_Rank(decay_linear(correlation(rank(low ), rank(adv30), 7.58555), 6.94024), \\n6.80584)) \\nAlpha#93:  (Ts_Rank(decay_linear(correlation(IndNeutralize(vw ap, IndClass.industry), adv81, \\n17.4193), 19.848), 7.54455) / rank(decay_linear(del ta(((close * 0.524434) + (vwap * (1 - \\n0.524434))), 2.77377), 16.2664))) \\nAlpha#94:  ((rank((vwap - ts_min(vwap, 11.5783)))^Ts_Rank(cor relation(Ts_Rank(vwap, \\n19.6462), Ts_Rank(adv60, 4.02992), 18.0926), 2.7075 6)) * -1) \\nAlpha#95:  (rank((open - ts_min(open, 12.4105))) < Ts_Rank((r ank(correlation(sum(((high + low) \\n/ 2), 19.1351), sum(adv40, 19.1351), 12.8742))^5), 11.7584)) \\nAlpha#96:  (max(Ts_Rank(decay_linear(correlation(rank(vwap), rank(volume), 3.83878), \\n4.16783), 8.38151), Ts_Rank(decay_linear(Ts_ArgMax( correlation(Ts_Rank(close, 7.45404), \\nTs_Rank(adv60, 4.13242), 3.65459), 12.6556), 14.036 5), 13.4143)) * -1) \\nAlpha#97:  ((rank(decay_linear(delta(IndNeutralize(((low * 0. 721001) + (vwap * (1 - 0.721001))), \\nIndClass.industry), 3.3705), 20.4523)) - Ts_Rank(de cay_linear(Ts_Rank(correlation(Ts_Rank(low, \\n7.87871), Ts_Rank(adv60, 17.255), 4.97547), 18.5925 ), 15.7152), 6.71659)) * -1)', metadata={'source': '101-Alpha-Formula.pdf', 'page': 13})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "771c2c71-5079-4f3c-83d1-5323446d04b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='13 \\n Alpha#74:  ((rank(correlation(close, sum(adv30, 37.4843), 15. 1365)) < \\nrank(correlation(rank(((high * 0.0261661) + (vwap *  (1 - 0.0261661)))), rank(volume), 11.4791))) \\n* -1) \\nAlpha#75:  (rank(correlation(vwap, volume, 4.24304)) < rank(c orrelation(rank(low), rank(adv50), \\n12.4413))) \\nAlpha#76:  (max(rank(decay_linear(delta(vwap, 1.24383), 11.82 59)), \\nTs_Rank(decay_linear(Ts_Rank(correlation(IndNeutral ize(low, IndClass.sector), adv81, \\n8.14941), 19.569), 17.1543), 19.383)) * -1) \\nAlpha#77:  min(rank(decay_linear(((((high + low) / 2) + high)  - (vwap + high)), 20.0451)), \\nrank(decay_linear(correlation(((high + low) / 2), a dv40, 3.1614), 5.64125))) \\nAlpha#78:  (rank(correlation(sum(((low * 0.352233) + (vwap * (1 - 0.352233))), 19.7428), \\nsum(adv40, 19.7428), 6.83313))^rank(correlation(ran k(vwap), rank(volume), 5.77492))) \\nAlpha#79:  (rank(delta(IndNeutralize(((close * 0.60733) + (op en * (1 - 0.60733))), \\nIndClass.sector), 1.23438)) < rank(correlation(Ts_R ank(vwap, 3.60973), Ts_Rank(adv150, \\n9.18637), 14.6644))) \\nAlpha#80:  ((rank(Sign(delta(IndNeutralize(((open * 0.868128)  + (high * (1 - 0.868128))), \\nIndClass.industry), 4.04545)))^Ts_Rank(correlation( high, adv10, 5.11456), 5.53756)) * -1) \\nAlpha#81:  ((rank(Log(product(rank((rank(correlation(vwap, su m(adv10, 49.6054), \\n8.47743))^4)), 14.9655))) < rank(correlation(rank(v wap), rank(volume), 5.07914))) * -1) \\nAlpha#82:  (min(rank(decay_linear(delta(open, 1.46063), 14.87 17)), \\nTs_Rank(decay_linear(correlation(IndNeutralize(volu me, IndClass.sector), ((open * 0.634196) + \\n(open * (1 - 0.634196))), 17.4842), 6.92131), 13.42 83)) * -1) \\nAlpha#83:  ((rank(delay(((high - low) / (sum(close, 5) / 5)),  2)) * rank(rank(volume))) / (((high - \\nlow) / (sum(close, 5) / 5)) / (vwap - close))) \\nAlpha#84:  SignedPower(Ts_Rank((vwap - ts_max(vwap, 15.3217)) , 20.7127), delta(close, \\n4.96796)) \\nAlpha#85:  (rank(correlation(((high * 0.876703) + (close * (1  - 0.876703))), adv30, \\n9.61331))^rank(correlation(Ts_Rank(((high + low) / 2), 3.70596), Ts_Rank(volume, 10.1595), \\n7.11408))) \\nAlpha#86:  ((Ts_Rank(correlation(close, sum(adv20, 14.7444), 6.00049), 20.4195) < rank(((open \\n+ close) - (vwap + open)))) * -1)', metadata={'source': '101-Alpha-Formula.pdf', 'page': 12})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cab4448-bbaf-4466-828b-a1edaafc96d2",
   "metadata": {},
   "source": [
    "## Create a graph state\n",
    "\n",
    "Our state will be a `dict`, We can access through any graph `node` as `state[keys]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a011dff7-ee0a-45c7-a78d-102f4bcc3ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Dict\n",
    "\n",
    "class AlphaRagState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the graph state of the alpha agent\n",
    "\n",
    "    Attributes:\n",
    "        keys: A dictionary where \n",
    "    \"\"\"\n",
    "    keys: Dict[str, any]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dbf322ba-b9da-414c-9537-a2e97e7834f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_llm = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21186055-aab0-44fd-9110-f2500d4c9678",
   "metadata": {},
   "source": [
    "## Define nodes for our workflow\n",
    "Create the following nodes:\n",
    "1. retrieve\n",
    "2. grade_documents\n",
    "3. generate\n",
    "4. transform_query\n",
    "5. prepare_for_final_grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a0742635-2b2b-43e7-a253-ec001cd208db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve node gets relevant documents based on a user query\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import Optional, List\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieves relevant documents\n",
    "\n",
    "    Args: \n",
    "        state (dict): current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to the state, documents, contains relevant documents to question \n",
    "    \"\"\"\n",
    "    print(\"-- RETRIEVING DOCUMENTS --\")\n",
    "    print(\"State: \", state)\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    relevant_docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "    return {\"keys\": { \"documents\": relevant_docs, \"question\": question}}\n",
    "    \n",
    "# grade_documents node\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECKING RELEVANCE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "\n",
    "    # Data model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score to check the relevance of a document to a question\"\"\"\n",
    "        binary_score: str = Field(description=\"Is document relevant to a question, 'yes' or 'no'. \")\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOpenAI(temperature=0, model=openai_llm, streaming=True)\n",
    "    \n",
    "    # Tool\n",
    "    grade_tool_oai = convert_to_openai_tool(grade)\n",
    "\n",
    "    # LLM with tool and enforce invocation\n",
    "    llm_with_tool = llm.bind(\n",
    "        tools=[grade_tool_oai],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"grade\"}}\n",
    "    )\n",
    "\n",
    "    # Parser\n",
    "    parser_tool = PydanticToolsParser(tools=[grade])\n",
    "    \n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm_with_tool | parser_tool\n",
    "\n",
    "    # Score\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = chain.invoke({\"question\": question, \"context\": d.page_content})\n",
    "        print(\"Score for current document: \", score)\n",
    "\n",
    "        grade = score[0].binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "\n",
    "    return {\"keys\": {\"documents\": filtered_docs, \"question\": question}}\n",
    "            \n",
    "# generate node for generating answer to questions\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "\n",
    "    # ALPHA_EXPRESSION_EXAMPLE = \"\"\"\n",
    "    # (-1 * delta((((close - low) - (high - close)) / (close - low)), 9))\n",
    "    # \"\"\"\n",
    "    class alpha_info(BaseModel):\n",
    "        \"\"\"This contains essential alpha information in the retrieved documents\"\"\"\n",
    "        alpha_number: str = Field(description=\"This is the label of alpha expression in the retrieved documents. For example: Alpha#1, Alpha#2, ...., Alpha#101\")\n",
    "        alpha_expression: str = Field(description=\"This contains alpha expression in retrieved documents.\")\n",
    "        alpha_explanation: str = Field(description=\"Explanation of the alpha expression retrieved. Use your internal knowledge\")\n",
    "\n",
    "    class alpha_response(BaseModel):\n",
    "        \"\"\"This model can either be None or a list of alpha_info instances\"\"\"\n",
    "        alpha_infos: Optional[List[alpha_info]] = None\n",
    "    \n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "    template= \"\"\"You are an assistant for question-answering tasks. \\n\n",
    "    Use the retrieved documents to answer the question. If you don't know the answer, just say that you don't know. \\n\n",
    "     \n",
    "    Here are the retrieved documents: \n",
    "    {context}\n",
    "    \n",
    "    Here is the user question: \n",
    "    {question}\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    )\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOpenAI(temperature=0, model=openai_llm, streaming=True)\n",
    "\n",
    "    # Tool\n",
    "    grade_tool_oai = convert_to_openai_tool(alpha_response)\n",
    "\n",
    "    # LLM with tool and enforce invocation\n",
    "    llm_with_tool = llm.bind(\n",
    "        tools=[grade_tool_oai],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"alpha_response\"}}\n",
    "    )\n",
    "\n",
    "    # Parser\n",
    "    parser_tool = PydanticToolsParser(tools=[alpha_response])\n",
    "\n",
    "    formatted_docs = format_docs(documents)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm_with_tool | parser_tool\n",
    "\n",
    "    # Run\n",
    "    generation = rag_chain.invoke({\"context\":formatted_docs, \"question\": question})\n",
    "    \n",
    "    print(\"Generated response to alpha query: \", generation)\n",
    "    return {\n",
    "        \"keys\": {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "    }\n",
    "\n",
    "# transform query node, to regenerate query based on \n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORMING QUERY---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "\n",
    "    # Create a prompt template with format instructions and the query\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are generating alpha trading strategy-relation question / query that is well optimized for retrieval. \\n \n",
    "        Look at the input and try to reason about the underlying sematic intent / meaning. \\n \n",
    "        Here is the initial question:\n",
    "        \\n ------- \\n\n",
    "        {question} \n",
    "        \\n ------- \\n\n",
    "        Formulate an improved alpha trading strategy question: \"\"\",\n",
    "        input_variables=[\"question\"],\n",
    "    )\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOpenAI(temperature=0, model=openai_llm, streaming=True)\n",
    "\n",
    "    # Prompt\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    better_question = chain.invoke({\"question\": question})\n",
    "\n",
    "    return {\"keys\": {\"documents\": documents, \"question\": better_question}}\n",
    "\n",
    "# prepare for final grade, \n",
    "def prepare_for_final_grade(state):\n",
    "    \"\"\"\n",
    "    Passthrough state for final grade.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): The current graph state\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---FINAL GRADE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    generation = state_dict[\"generation\"]\n",
    "\n",
    "    return {\n",
    "        \"keys\": {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d166b82-1eea-438b-8168-79383e4918fe",
   "metadata": {},
   "source": [
    "## Define edges for our workflow\n",
    "Create the following edges:\n",
    "1. decide_to_generate\n",
    "2. decide_generation_is_grounded_in_documents\n",
    "3. decide_generation_addresses_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e451d721-afed-452b-9cf8-cedb97e2c3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current state of the agent, including all keys.\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---DECIDE TO GENERATE OR TRANSFORM QUERY---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    filtered_documents = state_dict[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"---DECISION: TRANSFORM QUERY---\")\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def decide_generation_is_grounded_in_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current state of the agent, including all keys.\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---GRADE GENERATION based on DOCUMENTS---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    generation = state_dict[\"generation\"]\n",
    "\n",
    "    # Data model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score to check that answer is relevant to the documents.\"\"\"\n",
    "\n",
    "        binary_score: str = Field(description=\"Supported score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOpenAI(temperature=0, model=openai_llm, streaming=True)\n",
    "\n",
    "    # Tool\n",
    "    grade_tool_oai = convert_to_openai_tool(grade)\n",
    "\n",
    "    # LLM with tool and enforce invocation\n",
    "    llm_with_tool = llm.bind(\n",
    "        tools=[grade_tool_oai],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"grade\"}},\n",
    "    )\n",
    "\n",
    "    # Parser\n",
    "    parser_tool = PydanticToolsParser(tools=[grade])\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing whether an answer is grounded in / supported by a set of facts. \\n \n",
    "        Here are the facts:\n",
    "        \\n ------- \\n\n",
    "        {context} \n",
    "        \\n ------- \\n\n",
    "        Here is the answer: {generation}\n",
    "        Give a binary score 'yes' or 'no' to indicate whether the answer is grounded in / supported by a set of facts.\"\"\",\n",
    "        input_variables=[\"generation\", \"context\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm_with_tool | parser_tool\n",
    "\n",
    "    formatted_docs = format_docs(docs)\n",
    "\n",
    "    score = chain.invoke({\"generation\": generation, \"context\": formatted_docs})\n",
    "    grade = score[0].binary_score\n",
    "\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: SUPPORTED, MOVE TO FINAL GRADE---\")\n",
    "        return \"grounded\"\n",
    "    else:\n",
    "        print(\"---DECISION: NOT SUPPORTED, GENERATE AGAIN---\")\n",
    "        return \"not grounded\"\n",
    "\n",
    "\n",
    "def decide_generation_addresses_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation addresses the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current state of the agent, including all keys.\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    generation = state_dict[\"generation\"]\n",
    "\n",
    "    # Data model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score to check relevance of answer to question.\"\"\"\n",
    "\n",
    "        binary_score: str = Field(description=\"Useful score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOpenAI(temperature=0, model=openai_llm, streaming=True)\n",
    "\n",
    "    # Tool\n",
    "    grade_tool_oai = convert_to_openai_tool(grade)\n",
    "\n",
    "    # LLM with tool and enforce invocation\n",
    "    llm_with_tool = llm.bind(\n",
    "        tools=[grade_tool_oai],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"grade\"}},\n",
    "    )\n",
    "\n",
    "    # Parser\n",
    "    parser_tool = PydanticToolsParser(tools=[grade])\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing whether an answer is useful to resolve a question. \\n \n",
    "        Here is the answer:\n",
    "        \\n ------- \\n\n",
    "        {generation} \n",
    "        \\n ------- \\n\n",
    "        Here is the question: {question}\n",
    "        Give a binary score 'yes' or 'no' to indicate whether the answer is useful to resolve a question.\"\"\",\n",
    "        input_variables=[\"generation\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Prompt\n",
    "    chain = prompt | llm_with_tool | parser_tool\n",
    "    binary_result = answer_grader.invoke({\"question\": question,\"generation\": generation})\n",
    "    grade = binary_result[\"score\"]\n",
    "    \n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: USEFUL---\")\n",
    "        return \"useful\"\n",
    "    else:\n",
    "        print(\"---DECISION: NOT USEFUL---\")\n",
    "        return \"not useful\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d4fd38-2efd-4ecc-9907-2c57a7afc1f2",
   "metadata": {},
   "source": [
    "## Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c0ed5657-973a-496a-83c1-8a97c25675f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(AlphaRagState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
    "workflow.add_node(\"prepare_for_final_grade\", prepare_for_final_grade)  # passthrough\n",
    "\n",
    "# Build graph\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    decide_generation_is_grounded_in_documents,\n",
    "    {\n",
    "        \"grounded\": \"prepare_for_final_grade\",\n",
    "        \"not grounded\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"prepare_for_final_grade\",\n",
    "    decide_generation_addresses_question,\n",
    "    {\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"transform_query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f0ef83-8ed8-41f5-a819-6cc0bf0a48cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- RETRIEVING DOCUMENTS --\n",
      "State:  {'keys': {'question': 'Generate alpha information(s) based on a strategy \\n\\nYou are to respond with alpha information(alpha number, alpha expression and alpha explanation). \\n\\n\\nHere is the strategy: \\n\\nflow of funds strategy\\n\\nHere is an example of alpha expression: \\n\\n\\n(rank((open - (sum(vwap, 10) / 10))) * (-1 * abs(rank((close - vwap))))) \\n\\n((rank(Log(product(rank((rank(correlation(vwap, sum(adv10, 49.6054),\\n8.47743))^4)), 14.9655))) < rank(correlation(rank(vwap), rank(volume), 5.07914))) * -1) \\n\\n(max(rank(decay_linear(delta(((close * 0.369701) + (vwap * (1 - 0.369701))),\\n1.91233), 2.65461)), Ts_Rank(decay_linear(abs(correlation(IndNeutralize(adv81,\\nIndClass.industry), close, 13.4132)), 4.89768), 14.4535)) * -1) \\n\\n\\n'}}\n",
      "'Finished running: retrieve'\n",
      "---CHECKING RELEVANCE---\n",
      "Score for current document:  [grade(binary_score='no')]\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "Score for current document:  [grade(binary_score='yes')]\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "Score for current document:  [grade(binary_score='yes')]\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "Score for current document:  [grade(binary_score='no')]\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "Score for current document:  [grade(binary_score='no')]\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "Score for current document:  [grade(binary_score='yes')]\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "Score for current document:  [grade(binary_score='no')]\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "Score for current document:  [grade(binary_score='no')]\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "Score for current document:  [grade(binary_score='no')]\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "Score for current document:  [grade(binary_score='no')]\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "Score for current document:  [grade(binary_score='no')]\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "Score for current document:  [grade(binary_score='no')]\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "Score for current document:  [grade(binary_score='yes')]\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "Score for current document:  [grade(binary_score='no')]\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "Score for current document:  [grade(binary_score='no')]\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "Score for current document:  [grade(binary_score='no')]\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "Score for current document:  [grade(binary_score='no')]\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "Score for current document:  [grade(binary_score='no')]\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---DECIDE TO GENERATE OR TRANSFORM QUERY---\n",
      "---DECISION: GENERATE---\n",
      "'Finished running: grade_documents'\n",
      "---GENERATE---\n",
      "Generated response to alpha query:  [alpha_response(alpha_infos=[alpha_info(alpha_number='Alpha#102', alpha_expression='(rank(correlation(vwap, adv20, 10)) - rank(correlation(close, volume, 10)))', alpha_explanation='This alpha strategy measures the flow of funds by comparing the correlation between VWAP (Volume Weighted Average Price) and average daily volume over 20 days with the correlation between closing price and volume over 10 days. A higher rank indicates a stronger flow of funds into the stock.'), alpha_info(alpha_number='Alpha#103', alpha_expression='(rank(sum(volume, 5)) - rank(sum(volume, 20))) * (rank(close) - rank(open))', alpha_explanation='This alpha strategy captures the short-term flow of funds by comparing the sum of volumes over the last 5 days with the sum over the last 20 days, and then adjusting it by the difference in ranks between the closing and opening prices. It indicates whether there is an increasing or decreasing flow of funds in the short term.')])]\n",
      "---GRADE GENERATION based on DOCUMENTS---\n",
      "---DECISION: NOT SUPPORTED, GENERATE AGAIN---\n",
      "'Finished running: generate'\n",
      "---GENERATE---\n",
      "Generated response to alpha query:  [alpha_response(alpha_infos=None)]\n",
      "---GRADE GENERATION based on DOCUMENTS---\n",
      "---DECISION: NOT SUPPORTED, GENERATE AGAIN---\n",
      "'Finished running: generate'\n",
      "---GENERATE---\n",
      "Generated response to alpha query:  [alpha_response(alpha_infos=None)]\n",
      "---GRADE GENERATION based on DOCUMENTS---\n",
      "---DECISION: NOT SUPPORTED, GENERATE AGAIN---\n",
      "'Finished running: generate'\n",
      "---GENERATE---\n",
      "Generated response to alpha query:  [alpha_response(alpha_infos=[alpha_info(alpha_number='Alpha#1', alpha_expression='rank(correlation(vwap, volume, 5))', alpha_explanation='This alpha expression ranks the correlation between VWAP (Volume Weighted Average Price) and volume over a 5-day period. It is based on the flow of funds strategy, which assumes that higher trading volumes and VWAP correlations indicate significant fund flows.'), alpha_info(alpha_number='Alpha#2', alpha_expression='rank(delta(volume, 3)) * rank(delta(close, 3))', alpha_explanation='This alpha expression ranks the change in volume over the past 3 days and multiplies it by the rank of the change in closing price over the same period. It captures the flow of funds by identifying periods of significant volume and price changes.')])]\n",
      "---GRADE GENERATION based on DOCUMENTS---\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "from pprint import pprint\n",
    "\n",
    "ALPHA_EXPRESSION_EXAMPLE = \"\"\"\n",
    "(rank((open - (sum(vwap, 10) / 10))) * (-1 * abs(rank((close - vwap))))) \\n\n",
    "((rank(Log(product(rank((rank(correlation(vwap, sum(adv10, 49.6054),\n",
    "8.47743))^4)), 14.9655))) < rank(correlation(rank(vwap), rank(volume), 5.07914))) * -1) \\n\n",
    "(max(rank(decay_linear(delta(((close * 0.369701) + (vwap * (1 - 0.369701))),\n",
    "1.91233), 2.65461)), Ts_Rank(decay_linear(abs(correlation(IndNeutralize(adv81,\n",
    "IndClass.industry), close, 13.4132)), 4.89768), 14.4535)) * -1) \\n\n",
    "\"\"\"\n",
    "\n",
    "STRATEGY = \"Momentum\"\n",
    "query = f\"\"\"Generate alpha information(s) based on a strategy \\n\n",
    "You are to respond with alpha information(alpha number, alpha expression and alpha explanation). \\n\n",
    "\n",
    "Here is the strategy: \\n\n",
    "{STRATEGY}\n",
    "\n",
    "Here is an example of alpha expression: \\n\n",
    "{ALPHA_EXPRESSION_EXAMPLE}\n",
    "\"\"\"\n",
    "\n",
    "inputs = {\"keys\": {\"question\": query }}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Elapsed time: {end - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba012231-b6fc-49f6-9274-9ad4b3900c36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d271707-acd3-47b3-8928-6e48658ff356",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
