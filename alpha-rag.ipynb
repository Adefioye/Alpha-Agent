{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "989a6efe-8918-433f-888d-7f88a94e44cf",
   "metadata": {},
   "source": [
    "# PROBLEM STATEMENT\n",
    "The primary objective of this work is to create a RAG pipeline over the `101 alpha formulaic book` to extract alpha number, expression and explanation. Utimately, this will serve as a basis for creating an end-to-end alpha agent that will discover alpha signals over a knowledge base, create its python implementation, perform backtesting and trade live if expected performance pass a satisfactory threshold.\n",
    "\n",
    "# TODO\n",
    "- Use the __ChatGrok model__ to increase inference speed.\n",
    "- Use more capable model to generate alpha information.\n",
    "\n",
    "# CHALLENGES\n",
    "- Sometimes the model is unable to generate the alpha information due to OutputParserException.\n",
    "- The alpha expression and the alpha number returned do not match what is in the document(Hallucination). At this time, it is entirely possible to posit that the alpha expression are not even contained in the document. Perhaps, they are generated from internal knowledge of the model.\n",
    "\n",
    "# POSSIBLE SOLUTIONS TO CHALLENGES\n",
    "- Use more advanced LLM and embeddings method. Perhaps, this may help address the hallucination issue. I am refraining from using extraction approach as long-term, It is expected that we would have a rich and large repositories of papers, documents with alpha information. Also, the improved understanding of LLM models with advanced maths would help incredibly in this regard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "67400e0d-30f2-404a-bf67-d7e8119fec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Optional, add tracing in LangSmith\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Alpha agent project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "85064a5f-d5a5-49e9-bfce-906dcd485381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "def _set_if_undefined(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass(f\"Pass in {var}\")\n",
    "\n",
    "_set_if_undefined(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efc500eb-841d-4e13-93af-65e9dff201cf",
   "metadata": {},
   "source": [
    "## Ground an alpha retriever to document provided\n",
    "\n",
    "The goal of this work is to generate alpha expression and explanation based on the document provided. If response not relevant to document, we return not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b3c237-9a3a-44ed-b3f4-e00fa78474d3",
   "metadata": {},
   "source": [
    "Based on the workflow above, the following steps are followed:\n",
    "1. Based on a given `query`, we generate relevant documents.\n",
    "2. We then use a `binary score` to check whether each retrieved document is relevant or not.\n",
    "3. __If document is relevant__, we generate an answer. otherwise, we `re-write` the `query`.\n",
    "4. For the answer generated in 3, we check if the answer is grounded to the document(to reduce hallucination).\n",
    "5. __If no hallucination__, we then check if `answer` is relevant to the question. otherwise `repeat step 3`.\n",
    "6. __If answer is relevant to question__, return answer. otherwise, `repeat 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6d2df39b-caa3-48e0-a521-8013cfb33d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries (UNCOMMENT below)\n",
    "#!pip install pypdf gpt4all langchain langchain-core langchain-community chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9a80053e-15e7-41bb-84b8-a8c58963bf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "local_llm = 'llama3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2123399e-7ddf-4f43-8ffc-531923cd0da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "loader = PyPDFLoader(\"101-Alpha-Formula.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "doc_splits = splitter.split_documents(documents)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"alpha-doc-chroma\",\n",
    "    embedding=GPT4AllEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b74cfc5-c131-4d9d-9fff-47a3fca6c329",
   "metadata": {},
   "source": [
    "## Create a graph state\n",
    "\n",
    "Our state will be a `dict`, We can access through any graph `node` as `state[keys]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b91e873e-b14b-485d-8b74-ffeccf062d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Dict\n",
    "\n",
    "class AlphaRagState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the graph state of the alpha agent\n",
    "\n",
    "    Attributes:\n",
    "        keys: A dictionary where \n",
    "    \"\"\"\n",
    "    keys: Dict[str, any]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d349c7bd-520a-47d1-a94a-e190af7c803b",
   "metadata": {},
   "source": [
    "## Define nodes for our workflow\n",
    "Create the following nodes:\n",
    "1. retrieve\n",
    "2. grade_documents\n",
    "3. generate\n",
    "4. transform_query\n",
    "5. prepare_for_final_grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fd933e58-f85f-4f76-a987-58e33d511098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve node gets relevant documents based on a user query\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieves relevant documents\n",
    "\n",
    "    Args: \n",
    "        state (dict): current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to the state, documents, contains relevant documents to question \n",
    "    \"\"\"\n",
    "    print(\"-- RETRIEVING DOCUMENTS --\")\n",
    "    print(\"State: \", state)\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    relevant_docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "    return {\"keys\": { \"documents\": relevant_docs, \"question\": question}}\n",
    "    \n",
    "# grade_documents node\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECKING RELEVANCE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "    # prompt\n",
    "    prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing relevance \n",
    "    of a retrieved document to a user question. If the document contains keywords related to the user question, \n",
    "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \n",
    "    \n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\n",
    "     \n",
    "    Here is the retrieved document: \n",
    "    {document}\n",
    "    \n",
    "    Here is the user question: \n",
    "    {question}\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm | JsonOutputParser()\n",
    "\n",
    "    # Score\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        binary_result = chain.invoke({\"question\": question, \"document\": d.page_content})\n",
    "        print(\"Score result: \", binary_result)\n",
    "        grade = binary_result[\"score\"]\n",
    "\n",
    "        print(\"Grade: \", grade)\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "\n",
    "    return {\"keys\": {\"documents\": filtered_docs, \"question\": question}}\n",
    "            \n",
    "# generate node for generating answer to questions\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "\n",
    "    list_of_alpha_numbers = ['Alpha#1', 'Alpha#2', 'Alpha#3', 'Alpha#101']\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "    template= \"\"\"You are an assistant for question-answering tasks. \\n\n",
    "    Use the retrieved documents to answer the question. If you don't know the answer, just say that you don't know. \\n\n",
    "\n",
    "    Please provide an answer as a JSON with 3 keys: 'alpha_number', 'alpha_expression', and 'alpha_explanation'. \\n\n",
    "    Please ensure 'alpha_number' coincide with 'alpha_expression' as provided in the retrieved documents. \\n\n",
    "    Ensure that the 'alpha_explanation' is a description of the 'alpha_expression'. \\n\n",
    "\n",
    "    Here are examples of alpha numbers in the documents:\n",
    "    {list_of_alpha_numbers} \\n\n",
    "    \n",
    "    Here are 3 examples of alpha expression that can be extracted from documents:\n",
    "    ((-1 * rank((delta(close, 7) * (1 - rank(decay_linear((volume / adv20), 9)))))) * (1 + rank(sum(returns, 250)))) \\n\n",
    "    rank((-1 * ((1 - (open / close))^1))) \\n\n",
    "    (-1 * delta((((close - low) - (high - close)) / (close - low)), 9)) \\n\n",
    "     \n",
    "    Here are the retrieved documents: \n",
    "    {documents}\n",
    "    \n",
    "    Here is the user question: \n",
    "    {question}\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"documents\", \"list_of_alpha_numbers\"],\n",
    "    )\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "    # Post-processing\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | JsonOutputParser()\n",
    "\n",
    "    # Run\n",
    "    generation = rag_chain.invoke({\"documents\": documents, \"question\": question, \"list_of_alpha_numbers\": list_of_alpha_numbers})\n",
    "    \n",
    "    print(\"Generated response to alpha query: \", generation)\n",
    "    return {\n",
    "        \"keys\": {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "    }\n",
    "\n",
    "# transform query node, to regenerate query based on \n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORMING QUERY---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "\n",
    "    # Create a prompt template with format instructions and the query\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are generating alpha trading idea question / query that is well optimized for retrieval. \\n \n",
    "        Look at the input and try to reason about the underlying sematic intent / meaning. \\n \n",
    "        Here is the initial question:\n",
    "        \\n ------- \\n\n",
    "        {question} \n",
    "        \\n ------- \\n\n",
    "        Formulate an improved alpha trading idea question: \"\"\",\n",
    "        input_variables=[\"question\"],\n",
    "    )\n",
    "\n",
    "    # Grader\n",
    "    llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "    # Prompt\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    better_question = chain.invoke({\"question\": question})\n",
    "\n",
    "    return {\"keys\": {\"documents\": documents, \"question\": better_question}}\n",
    "\n",
    "# prepare for final grade, \n",
    "def prepare_for_final_grade(state):\n",
    "    \"\"\"\n",
    "    Passthrough state for final grade.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): The current graph state\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---FINAL GRADE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    generation = state_dict[\"generation\"]\n",
    "\n",
    "    return {\n",
    "        \"keys\": {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e1a345-a80c-4842-b324-0d7d8f40cd3c",
   "metadata": {},
   "source": [
    "## Define edges for our workflow\n",
    "Create the following edges:\n",
    "1. decide_to_generate\n",
    "2. decide_generation_is_grounded_in_documents\n",
    "3. decide_generation_addresses_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9686cd96-2ca3-4f58-8a9b-fcdd67fbb1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current state of the agent, including all keys.\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---DECIDE TO GENERATE OR TRANSFORM QUERY---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    filtered_documents = state_dict[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"---DECISION: TRANSFORM QUERY---\")\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def decide_generation_is_grounded_in_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current state of the agent, including all keys.\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---GRADE GENERATION based on DOCUMENTS---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    generation = state_dict[\"generation\"]\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing whether \n",
    "    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \n",
    "    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \n",
    "    single key 'score' and no preamble or explanation.\n",
    "    \n",
    "    Here are the facts:\n",
    "    {documents} \n",
    "\n",
    "    Here is the answer: \n",
    "    {generation}\n",
    "    \"\"\",\n",
    "    input_variables=[\"generation\", \"documents\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm | JsonOutputParser()\n",
    "\n",
    "    binary_result = chain.invoke({\"generation\": generation, \"documents\": documents})\n",
    "    grade = binary_result[\"score\"]\n",
    "\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: SUPPORTED, MOVE TO FINAL GRADE---\")\n",
    "        return \"grounded\"\n",
    "    else:\n",
    "        print(\"---DECISION: NOT SUPPORTED, GENERATE AGAIN---\")\n",
    "        return \"not grounded\"\n",
    "\n",
    "\n",
    "def decide_generation_addresses_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation addresses the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current state of the agent, including all keys.\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    generation = state_dict[\"generation\"]\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing whether an \n",
    "    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is \n",
    "    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "     \n",
    "    Here is the answer:\n",
    "    {generation} \n",
    "\n",
    "    Here is the question: {question}\n",
    "    \"\"\",\n",
    "    input_variables=[\"generation\", \"question\"],\n",
    "    )\n",
    "\n",
    "    answer_grader = prompt | llm | JsonOutputParser()\n",
    "    binary_result = answer_grader.invoke({\"question\": question,\"generation\": generation})\n",
    "    grade = binary_result[\"score\"]\n",
    "    \n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: USEFUL---\")\n",
    "        return \"useful\"\n",
    "    else:\n",
    "        print(\"---DECISION: NOT USEFUL---\")\n",
    "        return \"not useful\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cd2f95-5558-46b3-947f-539cc3333cdb",
   "metadata": {},
   "source": [
    "## Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "18ecbb35-0e3c-4847-abe7-32e4544b688a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(AlphaRagState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
    "workflow.add_node(\"prepare_for_final_grade\", prepare_for_final_grade)  # passthrough\n",
    "\n",
    "# Build graph\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    decide_generation_is_grounded_in_documents,\n",
    "    {\n",
    "        \"grounded\": \"prepare_for_final_grade\",\n",
    "        \"not grounded\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"prepare_for_final_grade\",\n",
    "    decide_generation_addresses_question,\n",
    "    {\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"transform_query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d9783d78-5a8e-4cbf-8f43-03e8a3d235b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- RETRIEVING DOCUMENTS --\n",
      "State:  {'keys': {'question': 'Generate alpha expression and explanation based on flow of funds strategy'}}\n",
      "'Finished running: retrieve'\n",
      "---CHECKING RELEVANCE---\n",
      "Score result:  {'score': 'yes'}\n",
      "Grade:  yes\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "Score result:  {'score': 'yes'}\n",
      "Grade:  yes\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "Score result:  {'score': 'yes'}\n",
      "Grade:  yes\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "Score result:  {'score': 'yes'}\n",
      "Grade:  yes\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---DECIDE TO GENERATE OR TRANSFORM QUERY---\n",
      "---DECISION: GENERATE---\n",
      "'Finished running: grade_documents'\n",
      "---GENERATE---\n",
      "Generated response to alpha query:  {'alpha_number': 'Alpha#101', 'alpha_expression': '((-1 * rank((delta(close, 7) * (1 - rank(decay_linear(volume / adv20), 9))))) * (1 + rank(sum(returns, 250))))', 'alpha_explanation': 'This alpha expression is based on the flow of funds strategy, which measures the rate of change in trading volume and adjusts it for market conditions. The expression uses a combination of moving averages and ranking functions to identify trends and patterns in the flow of funds data.'}\n",
      "---GRADE GENERATION based on DOCUMENTS---\n",
      "---DECISION: SUPPORTED, MOVE TO FINAL GRADE---\n",
      "'Finished running: generate'\n",
      "---FINAL GRADE---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: USEFUL---\n",
      "'Finished running: prepare_for_final_grade'\n",
      "Elpased time: 169.10185480117798s\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "from pprint import pprint\n",
    "inputs = {\"keys\": {\"question\": \"Generate alpha expression and explanation based on flow of funds strategy\"}}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Elpased time: {end - start}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7e67b3-9453-4daa-916c-c9d030895dd3",
   "metadata": {},
   "source": [
    "__Wrong example gotten without few shot examples__\n",
    "1. Generated response to alpha query:  [{'alpha_expression': 'Flow of Funds Alpha', 'explanation': 'The Flow of Funds Alpha strategy involves analyzing the movement of funds between different asset classes, sectors, or industries to identify trends and patterns. This approach can help identify alpha opportunities by capturing changes in investor sentiment, market conditions, and economic indicators.'}].\n",
    "   \n",
    "__Generation example after passing few shot examples without few shot examples__(Gave generic explanation of flow of fund strategy instead of `providing explanation to alpha expression`)\n",
    "3. Generated response to alpha query:  [{'alpha_expression': '-1 * rank(decay_linear(volume / adv20), 9)) * (1 + rank(sum(returns, 250)))', 'explanation': 'The flow of funds strategy is based on the idea that the movement of money in and out of a portfolio can be a valuable indicator of future stock performance. By analyzing the volume data and smoothing it out using the `decay_linear` function, we can create a signal that captures the momentum of the market.'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6021cd7-8d7f-41ed-baec-8eb900bb144e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
