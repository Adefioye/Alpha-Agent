{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fbbd458-e9d6-4607-b9e0-5ba4a07fb175",
   "metadata": {},
   "source": [
    "# Financial analyst agent\n",
    "\n",
    "The job of this agent is to build a financial report with investment analysis, business highlights, risk assessment and chart plotting and ultimately reporting all findings inside a PDF report for business leaders.\n",
    "\n",
    "#### Tasks to be performed\n",
    "- Create a financial analyst prompt\n",
    "- Define tools to be used by the model\n",
    "\n",
    "## TODO\n",
    "- Ensure to set `FMP_API_KEY`, `FINNHUB_API_KEY`, `SEC_API_KEY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffe5fef1-d9b8-414f-8d4a-13181b887e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# Now you can import the module\n",
    "from utils import register_keys_from_json\n",
    "\n",
    "CONFIG_API_KEY_PATH = \"../config_api_key\"\n",
    "\n",
    "# This runs important enviromental variables\n",
    "register_keys_from_json(CONFIG_API_KEY_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad20e83e-7a98-4a4d-867e-f09b26b0f343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Low latency is crucial for Large Language Models (LLMs) because it directly impacts the user experience, model performance, and overall efficiency of language-based applications. Here are some reasons why low latency is essential for LLMs:\\n\\n1. **Real-time Interaction**: LLMs are often used in applications that require real-time interaction, such as chatbots, virtual assistants, and language translation systems. Low latency ensures that the model responds quickly to user input, providing a seamless and engaging experience.\\n2. **Conversational Flow**: In conversational AI, latency can disrupt the natural flow of conversation. High latency can lead to awkward pauses, making the interaction feel unnatural and frustrating. Low latency helps maintain a smooth conversation, allowing users to engage more naturally with the model.\\n3. **Model Performance**: LLMs rely on complex algorithms and massive datasets to generate responses. High latency can lead to increased computational overhead, which can negatively impact model performance, accuracy, and reliability. Low latency enables the model to process requests efficiently, reducing the risk of errors and inaccuracies.\\n4. **Scalability**: As the number of users and requests increases, high latency can become a bottleneck, limiting the scalability of LLM-based applications. Low latency enables the model to handle a higher volume of requests, making it more suitable for large-scale deployments.\\n5. **User Experience**: High latency can lead to user frustration, abandonment, and a negative overall experience. Low latency ensures that users receive prompt responses, which is essential for building trust, satisfaction, and loyalty.\\n6. **Competitive Advantage**: In today's fast-paced digital landscape, low latency can be a key differentiator for LLM-based applications. By providing rapid responses, developers can gain a competitive edge, attract more users, and establish a strong market presence.\\n7. **Edge Computing**: With the increasing adoption of edge computing, low latency becomes even more critical. Edge computing involves processing data closer to the user, reducing latency and improving real-time performance. LLMs optimized for low latency can take full advantage of edge computing benefits.\\n8. **Safety-Critical Applications**: In safety-critical applications, such as autonomous vehicles or medical diagnosis, high latency can have severe consequences. Low latency ensures that critical information is processed and responded to quickly, reducing the risk of accidents or misdiagnosis.\\n9. **Cost Savings**: High latency can result in increased infrastructure costs, as more resources are required to handle the additional computational overhead. Low latency enables developers to optimize their infrastructure, reducing costs and improving overall efficiency.\\n10. **Future-Proofing**: As LLMs continue to evolve and become more sophisticated, low latency will become even more essential. By optimizing for low latency today, developers can future-proof their applications, ensuring they remain competitive and effective in the years to come.\\n\\nIn summary, low latency is vital for LLMs because it enables real-time interaction, conversational flow, model performance, scalability, user experience, competitive advantage, edge computing, safety-critical applications, cost savings, and future-proofing. By prioritizing low latency, developers can create more efficient, effective, and engaging language-based applications that meet the evolving needs of users.\", response_metadata={'token_usage': {'completion_tokens': 644, 'prompt_tokens': 33, 'total_tokens': 677, 'completion_time': 1.9315180349999999, 'prompt_time': 0.005904993, 'queue_time': None, 'total_time': 1.9374230279999998}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_7ab5f7e105', 'finish_reason': 'stop', 'logprobs': None}, id='run-57adb3b8-7422-4d45-a4a1-5b0ba56ce4b4-0', usage_metadata={'input_tokens': 33, 'output_tokens': 644, 'total_tokens': 677})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "chat = ChatGroq(\n",
    "    temperature=0,\n",
    "    model=\"llama3-70b-8192\"\n",
    ")\n",
    "\n",
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | chat\n",
    "chain.invoke({\"text\": \"Explain the importance of low latency for LLMs.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbe69fa-af7e-4c44-ab76-d99dfdd39102",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
